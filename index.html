<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta name="theme-color" content="#000000" />
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="Open-Sans.css">
  <link rel="stylesheet" href="index.css">
  <title></title>
  <script defer="defer" src="./static/js/main.cb41f6a5.js"></script>
  <link href="./static/css/main.4017e162.css" rel="stylesheet">
  <meta name="description"
        content="Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency">
  <title>Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency</title>
</head>

<body>

  <div id="root" class="column-flex">
    <div id="title-flex" class="column-flex">
      <h1> Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency </h1>
      <p> <b>Note: since this page contains many videos, please wait patiently for page loading.</b> </p>
      <small><span><b>TL;DR</b>: we propose an end-to-end audio-only conditioned video diffusion model named <b>Loopy</b>. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference, delivering more lifelike and high-quality results across various scenarios.</span></small>
      <div class='responsive-image-container'>
        <img src='image/teaser.png' alt='' />
      </div>
    </div>

    <div id="sections" class="column-flex">
      <!-- <p style="color:#700000"><i>(Note: all portrait images on this page are virtual, non-existing identities generated by StyleGAN2 or DALLÂ·E-3 (except for Mona Lisa). We are exploring visual affective skill generation for virtual, interactive characters, NOT impersonating any person in the real world. This is only a research demonstration and there's no product or API release plan. See also the bottom of this page for more of our Responsible AI considerations.) </i></p> -->
      <h3>Open-source Loopy Testset</h3>
        <p>
          Here we list the test set splits, specific times, and cropping areas for Loopy in the packaged file (<a href="https://loopyavataranony.github.io/download/testset.zip">link</a>) to facilitate comparisons with Loopy.
        </p>  
        <p>
          For the original videos: (1) original videos of RAVDESS can be downloaded via <a href="https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio">link</a> (2) original videos of CelebVHQ can be downloaded and processed according to the instructions at <a href="https://github.com/CelebV-HQ/CelebV-HQ">link</a>.
        </p>

      <h3>Rebuttal Videos</h3>
      <div class="video-slider">
        <video src="video/rebuttal_video_limitation.mp4"></video>
        <video src="video/rebuttal_video_expmov_det.mp4"></video>
        <video src="video/rebuttal_video_aba.mp4"></video>
        <video src="video/rebutall_video_expmov_control.mp4"></video>
      </div>
      <h3>Generated Videos</h3>
        <p>
          Loopy supports various visual and audio styles. It can generate vivid motion details from audio alone, such as non-speech movements like sighing, emotion-driven eyebrow and eye movements, and natural head movements. <br/>
          <b>*</b> Note that all results in this page use the first frame as <b>reference image</b> and conditioned on <b>audio only</b> without need of spatial conditions as templates.
        </p>
        <div class="video-slider">
          <video src="video/main_1.mp4"></video>
          <video src="video/main_2.mp4"></video>
          <video src="video/main_3.mp4"></video>
          <video src="video/main_4.mp4"></video>
          <video src="video/main_5.mp4"></video>
        </div>

      <h3>Motion Diversity</h3>
        <p>
          Loopy can generate motion-adapted synthesis results for the same reference image based on different audio inputs, whether they are rapid, soothing, or realistic singing performances.
        </p>
        <div class="video-slider">
          <video src="video/diverse_1.mp4"></video>
          <video src="video/diverse_5.mp4"></video>
          <video src="video/diverse_2.mp4"></video>
          <video src="video/diverse_6.mp4"></video>
          <video src="video/diverse_3.mp4"></video>
          <video src="video/diverse_7.mp4"></video>
          <video src="video/diverse_4.mp4"></video>
          <video src="video/diverse_8.mp4"></video>
        </div>
        
      <h3>Singing</h3>
        <p>Additional results demonstrating singing</p>
        <div class="video-slider">
          <video src="video/sing_1.mp4"></video>
          <video src="video/sing_2.mp4"></video>
          <video src="video/sing_3.mp4"></video>
          <video src="video/sing_4.mp4"></video>
          <video src="video/sing_5.mp4"></video>
          <video src="video/sing_6.mp4"></video>
        </div>

      <h3>More Video Results</h3>
        <p>Additional results about non-human realistic images</p>
        <div class="video-slider">
          <video src="video/nonhuman_1.mp4"></video>
          <video src="video/nonhuman_2.mp4"></video>
          <video src="video/nonhuman_3.mp4"></video>
          <video src="video/nonhuman_4.mp4"></video>
          <video src="video/nonhuman_5.mp4"></video>
        </div>

        <p>Loopy also supports input images with side profiles effectively</p>
        <div class="video-slider">
          <video src="video/side_1.mp4"></video>
          <video src="video/side_2.mp4"></video>
          <video src="video/side_3.mp4"></video>
          <video src="video/side_4.mp4"></video>
          <video src="video/side_5.mp4"></video>
        </div>

        <p>More results about realistic portrait inputs</p>
        <div class="video-slider">
          <video src="video/realistic_1.mp4"></video>
          <video src="video/realistic_2.mp4"></video>
          <video src="video/realistic_3.mp4"></video>
          <video src="video/realistic_4.mp4"></video>
          <video src="video/realistic_5.mp4"></video>
        </div>

      <h3>Comparison with Recent Methods</h3>
        <div class="video-container">
          <video controls playsInline src="video/comparison_1.mp4"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/comparison_2.mp4"></video>
        </div>

      <h3>Ethics Concerns</h3>
        <p>
          The purpose of this work is only for research. The images and audios used in these demos are from public sources.
        </p>

      
      <br/>
      <br/>
    </div>
  </div>
  <script src="index.js"></script>
  <script>
    function comming_soon_click() {
      alert('Comming soon!');
    }
    function TBD_click() {
      alert('TBD');
    }
  </script>
</body>



</html>
